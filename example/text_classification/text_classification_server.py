import asyncio

from bert4keras.backend import keras, set_gelu
from bert4keras.models import build_transformer_model
from bert4keras.optimizers import Adam, extend_with_piecewise_linear_lr
from bert4keras.snippets import sequence_padding
from bert4keras.tokenizers import Tokenizer
from grpclib.server import Server, Stream
from grpclib.utils import graceful_exit
from keras.layers import Lambda, Dense

from text_classification_grpc import GreeterBase
# generated by protoc
from text_classification_pb2 import TextClassifyRequest, TextClassifyReply

set_gelu('tanh')  # 切换gelu版本

d = []
max_length = 128
batch_size = 64
num_classes = 5
label_index = 2
config_path = '../autodl-nas/chinese_rbt3_L-3_H-768_A-12/bert_config_rbt3.json'
checkpoint_path = None
dict_path = '../autodl-nas/chinese_rbt3_L-3_H-768_A-12/vocab.txt'

# 建立分词器
tokenizer = Tokenizer(dict_path, do_lower_case=True)
# 加载预训练模型
bert = build_transformer_model(
    config_path=config_path,
    checkpoint_path=checkpoint_path,
    model='bert',
    return_keras_model=False,
)

output = Lambda(lambda x: x[:, 0], name='CLS-token')(bert.model.output)
output = Dense(
    units=num_classes,
    activation='softmax',
    kernel_initializer=bert.initializer
)(output)

model = keras.models.Model(bert.model.input, output)

# 派生为带分段线性学习率的优化器。
# 其中name参数可选，但最好填入，以区分不同的派生优化器。
AdamLR = extend_with_piecewise_linear_lr(Adam, name='AdamLR')

model.compile(
    loss='sparse_categorical_crossentropy',
    # optimizer=Adam(1e-5),  # 用足够小的学习率
    optimizer=AdamLR(learning_rate=1e-4, lr_schedule={
        1000: 1,
        2000: 0.1
    }),
    metrics=['accuracy'],
)

model.load_weights('chinese_rbt3_best_model_' + str(label_index) + '.weights')


class Greeter(GreeterBase):

    async def SayTextClassify(self, stream: Stream[TextClassifyRequest, TextClassifyReply]) -> None:
        request = await stream.recv_message()
        assert request is not None
        message = f'Hello, {request.text}!'
        token_ids, segment_ids = tokenizer.encode(request.text, maxlen=max_length)
        # batch_token_ids = sequence_padding(token_ids)
        # batch_segment_ids = sequence_padding(segment_ids)
        y_pred = model.predict([sequence_padding([token_ids]), sequence_padding([segment_ids])]).argmax(axis=1)
        # print(y_pred)
        y_pred = y_pred[0]
        # print(y_pred)

        await stream.send_message(TextClassifyReply(label=message, score=str(y_pred)))


async def main(*, host: str = '127.0.0.1', port: int = 50052) -> None:
    server = Server([Greeter()])
    # Note: graceful_exit isn't supported in Windows
    with graceful_exit([server]):
        await server.start(host, port)
        print(f'Serving on {host}:{port}')
        await server.wait_closed()


if __name__ == '__main__':
    asyncio.run(main())
